\section{Introduction}
\label{sec:introduction}

Social media has become a ubiquitous part of everyday life. The amount of data being published through these services contains valuable potential information which can be exploited by algorithms and put to good 
in order to provide new and innovative services to the users~\cite{ref1}. However, extracting the semantics from the data is generally a hard problem and user provided information could aid to better contextualize
and infer their meaning.

One such category of data are photos published on social networks which are generally associated with a description and, some hashtags that label them. The latter could be personal words that a person associate 
to a certain photo (e.g. feelings, person names, places) but they could also be used in order to describes the content of the photo. All these pieces of information can be really useful to train algorithms and 
to create datasets used in order to recognize images: in fact, these data, which are freely available, are posted by a person that manually label and describe a specific photo.

However, we argue that user provided hashtags are often imprecise and user-subjective. As an alternative one could rely on image recognition services to automatically tag images prior to sharing them on social networks 
sites. Image recognition services process an image and return a set of labels associated to that picture. Results vary in precision depending on the 
photo quality, subject and also on the algorithm being employed. Indeed, at the current state of the art, image 
recognition algorithms cannot be as precise as a human to describe a picture. One of the opportunities to take advantage of this context is to 
embed in an application the possibility of taking photos which are processed by some image recognition service. The result of this process can be displayed 
to the user, who can choose only the subset of tags that she thinks are truly correlated to her picture: in that way users can automatically associate some 
tags to their photos and share them on social networks. On the other hand, using a human to check the result, you could evaluate how much a specific 
service works good, training your own dataset or algorithm.

Following this idea, we designed and developed \textit{GHio-Ca} (\textit{Giving 
Hashtags In Order to Classify Automatically}), an Android application that allows people to take photos or choose pictures which are automatically 
processed by some image recognition service. In particular, in order to achieve our purpose, we rely on the 
following services: (a) Computer Vision API by Microsoft Azure \cite{Microsoft}
, (b) Visual Recognition by IBM Watson \cite{IBM}, (c) Google Reverse Image 
Search \cite{Google}, (d) OCR Space \cite{OCR} and (e) Imagga \cite{Imagga}.

Our application was designed with quality of service and usability requirements in mind. 

This paper is organized as follows: Section~\ref{sec:background} describes how 
artificial intelligence can perform image processing based on pattern matching 
and neural networks, while Section~\ref{sec:related} provides an overview of 
the current state of the art of Computer Vision applications. 
Section~\ref{sec:ghioca} describes how the application is designed and 
implemented, while Section~\ref{sec:issues} lists the problems we faced while 
developing our application. Finally, in Section~\ref{sec:results} we compare 
different image recognition APIs and in Section~\ref{sec:future} we propose 
some future developments for our application. Finally, Section~\ref{sec:conclusion} 
provides conclusions about our work.