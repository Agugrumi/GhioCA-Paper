\section{GHio-Ca}
\label{sec:ghioca}
In this section we provide some salient features implemented in our
application, mainly regarding architectural and implementative choices that
we made during its development.

\subsection{Application description}
GHio-Ca is an Android application that gives to a user the possibility to make
image recognition on her pictures. This application embed some camera features
to make possible taking a photo and recognizing its content, but it also allows
to pick a picture from the local storage and start the classification process
on it. We imposed as minimum API level 21 (Android Lollipop).
GHio-Ca is specifically designed to recognize objects and text: the only
limitation is that the user has to choose it before starting the recognition
process from the hamburger menu placed in the \texttt{MainActivity}.
The overall application is mainly composed by four activities.

\subsubsection{\texttt{SplashActivity}}
This view (Figure~\ref{fig:splash}) is needed to obtain from the user
the necessary permissions (network access, camera access, access to  the
storage).
\begin{figure}[h]
    \centering
    \includegraphics[width=0.2\textwidth]{../img/splash}
    \caption{Screenshot of \texttt{SplashActivity} during authorization request}
    \label{fig:splash}
\end{figure}

\subsubsection{\texttt{CameraPreviewActivity}}
This activity (Figure~\ref{fig:mainActivity}) shows to the user a camera
interface  to take photos, turn on/off the flash, switch between back and
frontal camera (if present) and to access to the gallery in order to pick a file
instead of taking a picture. In the top left corner there is the hamburger
menu: it allows the user to choose the size of the picture she took, it can remind 
to the user to turn on the Wi-Fi sensor at every access and it allows to switch 
from the image recognition functionality to the Optical Character Recognition (OCR)
feature and vice-versa.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.30\textwidth]{../img/main_activity}
    \caption{Screenshots of \texttt{CameraPreviewActivity}, with the hamburger
menu open and closed}
    \label{fig:mainActivity}
\end{figure}

\subsubsection{\texttt{ResultActivity}}
There are two different activities (Figure~\ref{fig:imageResultActivity}),
depending on the type of image recognition chosen:
\begin{itemize}
        \item If the ``reverse image search'' functionality is enabled, then
\texttt{ResultActivity} contains a photo thumbnail, a description and a list of
tags. Any tag can be deselected by the user if it does not fit the image.
    \item If ``optical character recognition'' is activated,
\texttt{ResultActivity} displays the photographed text and its OCR result.
The user is able to pick another language for the translation from a drop down menu.
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.18\textwidth]{../img/image_result_activity}
    \caption{Screenshot of \texttt{ResultActivity} for image recognition}
    \label{fig:imageResultActivity}
\end{figure}

\subsection{Libris}
Libris\footnote{\url{https://github.com/Augugrumi/libris}} (Library for Reverse
Image Search) is a library that we developed in order simplify the image 
recognition process. The library aims to make easier making calls to 
the different services that we used, in order to fulfill the image or 
character recognition, defining an interface for the results.

All the requests return a response in \texttt{JSON} format, which is parsed.
The more relevant fields are embedded in a object, different for every
service, which is returned as result. If some error occurs during the request an
\texttt{Exception} is thrown.

For image recognition Libris gives the possibility to use all the services
listed in Section~\ref{sec:introduction}, except for Free OCR. In order to
provide the Google Reverse Search Image service, we programmatically search on
Google the image, choosing the best returned result.

For Optical Character Recognition, Libris gives the chance to use: Watson OCR (still in
beta), Azure OCR and OCR Space (free version).

\subsection{CameraFragment}
CameraFragment\footnote{\url{https://github.com/Augugrumi/CameraFragment}} is a
library that simplify the camera usage in Android. It is open source and freely 
available on Github. Because of some issues in the library, mainly caused by 
the wrong management of Android \texttt{Camera1} and \texttt{Camera2}, and due 
to the fact that some parts of that library differed from the one expected in 
GHio-Ca, we decided to modify that project, alligning it to our needs.

\subsection{Architecture}
GHio-Ca is principally composed by three layers (Figure~\ref{fig:architecture}):
(i) the first one manages network connectivity, uploads of photos to the server
and makes requests to different services in order to make image recognition
or the translation of certain pieces of text; (ii) the camera layer, that
manages the photo capturing process and storage on the devices, and (iii) the
layer that connects all the pieces together.

These layers are maintained as loose coupled as possible, in order to make it
easier to change the used services without making important changes to the overall
application (e.g changing the process of capturing a photo without modifying the
networking layer).

\begin{figure}[h]
    \centering
    \includegraphics[width=0.50\textwidth]{../img/ghioca_macro_component}
    \caption{GHio-Ca architecture}
    \label{fig:architecture}
\end{figure}

Since we need to make API calls to recognise an image, we implemented a
utility class that offers the possibility to create \texttt{AsyncTask}s,
different for each service, that will be executed on a thread pool (in order to
make possible to fulfil more tasks at same time). Each of those
\texttt{AsyncTask}s must be associated to a listener, in order to collect all
the results of the search. Practically, classes that implement the interface
\texttt{Listener} expose methods \texttt{onStart()}, \texttt{onFailure()} and
\texttt{onSuccess()}, that are called from the \texttt{AsyncTask}. The first one
is called in \texttt{onPreExecute()} method, instead, \texttt{onFailure()} is 
invoked in \texttt{onPostExecute()} if during the \texttt{doInBackground()} 
method some error occurred. The latter one is called if the 
\texttt{doInBackground()} method is executed without any issue, during 
\texttt{onPostExecute()}.

Thanks to the possibility of detecting failures in image recognition process - 
and as a result of using more services - we implemented some robustness feature 
in our application, enabling it to return an error to the user only if all 
services fail. The scenario where the application could start the image 
recognition process, but it would not be able to finish it, could still occurs 
(e.g. caused by a network fault).

If no errors occur, we decided to skim results based on the probability that a
word (or group of words) could be correlated to the user picture. So, if the
service provides this feature, we only take result that have an high
probability, setting the threshold to 70\% - we set that value empirically. If 
the service does not provide a probability, it generally divides results in two 
sets based on the fact that a certain word could be more or less correlated to 
the picture. In that case, we present to the user only the more related results.

The duty of this layer goes beyond the sole responsibility of searching the 
image, carrying on the process to upload and recognise the picture.
Due to limitations of some services and to reduce network usage, we decided 
to upload the photo to a server, and send the URL of the image to different 
services.
With this approach, we reduce bandwidth usage: the user needs to upload the
photo only once and she can take advantage of the URL to make the recognition 
with more services.

The camera layer, instead, has only to manage the photo taking process, in order
to make possible to take a photo and save it on the users' smartphone. These
activities are fulfilled using the Camera Fragment library. It uses a fragment
in order to show to the user a preview of the photo and, when the user tap on
the button to take photos, saves the image in a specific folder using a
listener (in our case, the GHio-Ca folder, inside Android default Picture
folder). To be able to recognise the photo taken, we decided to create a
\texttt{Listener} that has to check this process and, when the image is 
successfully saved on the smartphone, it starts the image recognition process.
This layer has also to give the user the ability to select the camera (frontal
or back), to manage flash (turn on/off or choose automatic option) and to enable
the user to choose the size of the photos.

The last layer is the one that has to connect the previous two layers. In fact,
this layer has to create listeners both for the camera and the search. It has
to remember user's search option choose in order to create the right kind of
\texttt{Listener}, \texttt{AsyncTask} for the search and in order to arrange the 
results that are displayed to the user. Moreover, it manages the application
sharing process: as a matter of fact, after a research, the application gives 
to the user the possibility to share the photo on different social networks and
through e-mails. For some of them (Facebook, Twitter, Instagram, Whatsapp,
Linkedin) there is a specific implementation, if the usage of a API is needed;
for the others, the default Android \texttt{IntentChooser} is used.
